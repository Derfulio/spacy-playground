# Wrapping up

## Mes nouvelles comp√©tences Spacy

### Chapitre 1
**1. Extraction de caract√©ristiques linguistiques : partie de discours, d√©pendances, entit√©s nomm√©es**

  Dans le premier chapitre, j'ai appris comment extraire des caract√©ristiques linguistiques comme les √©tiquettes de partie de discours, les d√©pendances syntaxiques et les entit√©s nomm√©es.

**2. Travail avec des pipelines pr√©-entra√Æn√©s**

  J'ai appris comment travailler avec des pipelines pr√©-entra√Æn√©s.

**3. Recherche de mots et de phrases selon des r√®gles de correspondance avec [Matcher](https://spacy.io/api/matcher) et [PhraseMatcher](https://spacy.io/api/phrasematcher)**

  J'ai appris √† √©crire des r√®gles de correspondances puissantes pour extraire des mots et des phrases en utilisant le [matcher](https://demos.explosion.ai/matcher?) et le phrase matcher de spaCy.
  Un autre matcher √† tester: le [DependencyMatcher](https://spacy.io/api/dependencymatcher).
  Mes questions:
  - Peut-on matcher le contexte gauche (look-behind)?
  - Peut-on matcher le contexte droit (look-ahead)?
  - Peut-on matcher des sous-groupes ($1, $2, $3 etc)?
    - ---> Bref est-ce que les regexp √©tendus fonctionnent et peuvent se combiner aux capacit√©s du matcher.
    - ---> https://spacy.io/usage/rule-based-matching: When using the REGEX operator, keep in mind that it operates on single tokens
    - Donc non...
    MAIS! Dans ce genre de cas on peut faire appel √† la librairie re.
  - LIMITE: on n'a forc√©ment plus acc√®s au token et son contenu.
  - Xelda me manque... car on pouvait faire du lookahead et lookbehind ET on pouvait matcher le lemme, le pos et tout ce qui a √©t√© extrait √† chaque couche d'extraction.
  - Un outil qui n'est plus maintenu qui permet de faire un peu des 2: https://github.com/clips/pattern/wiki/pattern-search

  

### Chapitre 2

**1. Meilleures pratiques pour l'emploi des structures de donn√©es Doc, Token Span, Vocab, Lexeme**

  Le chapitre 2 √©tait consacr√© √† l'extraction d'informations, et j'ai appris comment travailler avec les structures de donn√©es, les Doc, Token et Span, ainsi qu'avec le Vocab et les entr√©es lexicales.

**2. Recherche de similarit√©s s√©mantiques avec les vecteurs de mots**

  J'ai aussi utilis√© spaCy pour pr√©dire des similarit√©s s√©mantiques en utilisant des vecteurs de mots.

### Chapitre 3

**1. √âcriture de composants de pipeline avec des extensions d'attributs**

  Dans le chapitre 3, j'ai acquis des connaissances plus compl√®tes sur le pipeline de spaCy, et j'ai appris √† √©crire mes propres composants de pipeline personnalis√©s qui modifient le doc.

**2. Changement d'√©chelle des pipelines spaCy pour les rendre rapides**

  J'√©galement cr√©√© mes propres extensions d'attributs personnalis√©es pour des docs, des tokens et des spans, et j'ai appris √† traiter des flux et √† rendre des pipelines plus rapides.

### Chapitre 4

**1. Cr√©ation de donn√©es d'entra√Ænement pour les mod√®les statistiques de spaCy**

  Enfin, dans le chapitre 4, j'ai appris √† entra√Æner et √† actualiser les mod√®les statistiques de spaCy, sp√©cifiquement l'entity recognizer.

**2. Entra√Ænement et actualisation des mod√®les de r√©seaux de neurones de spaCy avec de nouvelles donn√©es**

  J'ai appris quelques trucs utiles sur la mani√®re de cr√©er des donn√©es d'entra√Ænement, et sur la mani√®re de concevoir ton sch√©ma de labellisation pour obtenir les meilleurs r√©sultats.
  
## D'autres choses √† faire avec spaCy

- [Entra√Ænement et actualisation](https://spacy.io/usage/training) d'autres composants de pipeline
  - Etiqueteur de partie de discours
  - Analyseur de d√©pendances
  - Classificateur de texte (ne fait pas partie des mod√®les pr√©entrain√©s, mais peut y √™tre ajout√© pour les compl√©ter)
  - [Personnalisation du tokeniseur](https://spacy.io/usage/linguistic-features#tokenization)
    - Ajout de r√®gles et d'exceptions pour scinder diff√©remment le texte: on n'est pas oblig√© d'accepter la tokenisation tel quel, on peut l'adapter
  - [Ajout ou am√©lioration du support pour d'autres langues](https://spacy.io/usage/linguistic-features)
    - Plus de 60 langues actuellement
    - Marge de progression importante pour des am√©liorations et plus de langues
    - Possibilit√© d'entra√Æner des mod√®les pour d'autres langues

## A √©tudier: Spacy Projects

[Spacy Projects](https://github.com/explosion/projects) permet de g√©rer et de partager des workflows spaCy de bout en bout pour diff√©rents cas d'utilisation et domaines, et d'orchestrer l'entrainement, le packaging et le d√©ploiement de pipelines personnalis√©s. On peut commencer par cloner un mod√®le de projet pr√©d√©fini, l'adapter √† nos besoins, charger nos donn√©es, former un pipeline, l'exporter sous forme de package Python, t√©l√©charger nos r√©sultats vers un stockage distant et partager nos r√©sultats avec notre √©quipe.

### üóÉ Categories

| Name                                                                         | Description                                                                                                                                                                                                                                                                                                                                                                          |
| ---------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| [`pipelines`](https://github.com/explosion/projects/tree/v3/pipelines)       | Templates pour l'entra√Ænement des pipelines NLP avec diff√©rents composants sur diff√©rents corpus.                                                                                                                                                                                                                                                                                    |
| [`tutorials`](https://github.com/explosion/projects/tree/v3/tutorials)       | Templates qui traitent de bout en bout un cas d'utilisation NLP sp√©cifique. Notamment: [ner_double](https://github.com/explosion/projects/tree/v3/tutorials/ner_double) pour la gestion de pipelines avec 2 mod√®les ner et [ner_drugs](https://github.com/explosion/projects/tree/v3/tutorials/ner_drugs) un projet montrant la synergie possible entre prodigy pour annoter et spacy|
| [`integrations`](https://github.com/explosion/projects/tree/v3/integrations) | Templates pr√©sentant des int√©grations avec des biblioth√®ques et des outils tiers pour la gestion de nos donn√©es et de nos exp√©riences, l'it√©ration sur des d√©mos et des prototypes et l'envoi de nos mod√®les en production.                                                                                                                                                          |
| [`benchmarks`](https://github.com/explosion/projects/tree/v3/benchmarks)     | Templates pour reproduire les crit√®res de r√©f√©rence de Spacy et produire des r√©sultats quantifiables faciles √† comparer avec d'autres syst√®mes ou versions de spaCy                                                                                                                                                                                                                  |
| [`experimental`](https://github.com/explosion/projects/tree/v3/experimental) | Workflows exp√©rimentaux et autres travaux √† la pointe√† utiliser √† nos risques et p√©rils.                                                                                                                                                                                                                                                                                             |

Consulte le site web pour plus d'info et la documentation

üëâ [spacy.io](https://spacy.io)

Merci et √† bient√¥t ! üëã

