"""
Vecteurs de mots et similarit√© s√©mantique
Objectif:
Dans cette le√ßon, tu vas apprendre √† utiliser spaCy pour pr√©dire √† quel point des documents,
des spans ou des tokens sont similaires les uns avec les autres.

Vecteurs de mots et similarit√© s√©mantique
Comparaison de similarit√© s√©mantique
    - spaCy peut comparer deux objets et pr√©dire leur similarit√©
    - Doc.similarity(), Span.similarity() et Token.similarity()
    - Accepte un autre objet et retourne un score de similarit√© (de 0 √† 1)
    - Important : n√©cessite un pipeline qui inclut les vecteurs de mots, par exemple:
        ‚úÖ fr_core_news_md (moyen)
        ‚úÖ fr_core_news_lg (grand)
        üö´ PAS fr_core_news_sm (petit)
"""
import spacy

# Charge un plus grand pipeline avec les vecteurs
nlp = spacy.load("fr_core_news_md")

# Compare deux documents
doc1 = nlp("J'aime la pizza")
doc2 = nlp("J'aime la quiche lorraine")
print(doc1.similarity(doc2))
# Ici, la pr√©diction est un score plut√¥t √©lev√© de similarit√© de 0,96 pour "J'aime la pizza" et "J'aime la quiche lorraine".
# 0.9686798359892211

# Cela fonctionne aussi avec les tokens.

# Compare deux tokens
doc = nlp("J'aime la pizza et la quiche")
token1 = doc[3]
token2 = doc[6]
print(token1.similarity(token2))
# Selon les vecteurs de mots, les tokens "pizza" et "quiche" sont relativement similaires, et obtiennent un score de 0,62.
# 0.62105

# Compare un document avec un token
doc = nlp("J'aime la pizza")
token = nlp("savon")[0]
# Ici, le score de similarit√© est assez bas et les deux objets sont consid√©r√©s assez peu similaires.
print(doc.similarity(token))
# 0.12344265753392583

# Compare un span avec un document
span = nlp("J'aime la pizza et les p√¢tes")[3:7]
doc = nlp("McDonalds vend des hamburgers")
# Le score retourn√© ici est 0,62, donc il y a une forme de similarit√©.
print(span.similarity(doc))
# 0.6186440160069984

# Comment spaCy pr√©dit la similarit√© ?
## La similarit√© est d√©termin√©e en utilisant des vecteurs de mots
## Des repr√©sentations multi-dimensionnelles de la signification des mots
## G√©n√©r√©es avec un algorithme comme Word2Vec et beaucoup de textes
## Peuvent √™tre ajout√©s aux pipelines de spaCy
## Par d√©faut : similarit√© cosinus, mais peut √™tre modifi√©e
## Les vecteurs des Doc et Span sont par d√©faut la moyenne des vecteurs des tokens
## Les phrases courtes sont meilleures que les longs documents comportant de nombreux mots non pertinents

doc = nlp("J'ai une banane")
# Acc√®de au vecteur via l'attribut token.vector
print(doc[3].vector)
# Pour se faire une id√©e, voici un exemple montrant √† quoi ressemblent ces vecteurs.
# D'abord, nous chargeons √† nouveau le pipeline moyen, qui comporte des vecteurs de mots.
# Ensuite, nous pouvons traiter un texte et chercher le vecteur d'un token en utilisant l'attribut .vector.
# Le r√©sultat est un vecteur √† 300 dimensions du mot "banane".

# La similarit√© depend du contexte d'application
# Utile pour de nombreuses applications : syst√®mes de recommandations, rep√©rage de doublons etc.
# Il n'y a pas de d√©finition objective de "similarit√©"
# Cela d√©pend du contexte et des besoins de l'application
doc1 = nlp("J'aime les chats")
doc2 = nlp("Je d√©teste les chats")

print(doc1.similarity(doc2))
# Les vecteurs de mots de spaCy attribuent par d√©faut un score √©lev√© de similarit√© entre
# "J'aime les chats" et "Je d√©teste les chats".
# Cela parait logique, car les deux textes expriment des sentiments √† propos des chats.
# Mais dans un contexte d'application diff√©rent,
# tu pourrais vouloir consid√©rer les phrases comme √©tant tr√®s dissemblables,
# parce qu'elles expriment des sentiments oppos√©s.

# Traite le texte
doc = nlp("Deux bananes en pyjamas")

# Obtiens le vecteur pour le token "bananes"
bananas_vector = doc[1].vector
print(bananas_vector)

doc1 = nlp("Le temps est au beau fixe")
doc2 = nlp("Le ciel est clair")

# Obtiens la similarit√© entre doc1 et doc2
similarity = doc1.similarity(doc2)
print(similarity)

doc = nlp("t√©l√© et livres")
token1, token2 = doc[0], doc[2]

# Obtiens la similarit√© entre les tokens "t√©l√©" et "livres"
similarity = token1.similarity(token2)
print(similarity)

doc = nlp(
    "C'√©tait un super restaurant. Ensuite nous sommes all√©s dans un bar vraiment sympa."
)

# Cr√©e des spans pour "super restaurant" et "bar vraiment sympa"
span1 = doc[3:5]
span2 = doc[12:15]

# Obtiens la similarit√© entre les spans
similarity = span1.similarity(span2)
print(similarity)

# Les similarit√©s ne sont pas *toujours* aussi
# probantes. Si tu commences √† d√©velopper s√©rieusement des applications de NLP qui
# exploitent la similarit√© s√©mantique, tu pourrais vouloir entrainer des vecteurs
# sur tes propres donn√©es,ou ajuster l'algorithme de similarit√© s√©mantique.
